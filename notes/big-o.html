<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" href="../assets/images/favicon.ico"><link rel="stylesheet" href="../assets/css/notes.css" type="text/css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Karla:400,400italic,700" type="text/css"><link rel="stylesheet" href="https://dl.dropboxusercontent.com/u/14218448/assets/prism.css" type="text/css"><title></title></head><body><div class="content"><main><article><h1>ELI5: Big O</h1><p><em>Sarah Lim</em>  (slim at northwestern dot edu)<br>24 April 2016</p>
<blockquote>
<p><strong>Note:</strong> A previous version of this article incorrectly ordered $log(n)$ as growing faster than $n$. This has since been fixed.</p>
</blockquote>
<h2>Measuring performance</h2><p>To measure an algorithm&#39;s efficiency, one intuitive idea is to consider the amount of time it takes to solve a problem (transform the input into the desired output).</p>
<p>Unfortunately, it&#39;s difficult to measure the <em>exact</em> runtime of an algorithm. In practice, runtimes are influenced by a lot of factors unrelated to the algorithm itself. For instance, the same function might run quickly on a state-of-the-art gaming PC, and much more slowly on an ancient Windows 98 machine.</p>
<p>So instead of obsessing over nanosecond differences in runtime, computer scientists use <strong>complexity analysis</strong> to classify algorithms  according to their approximate efficiency. Complexity analysis investigates <em>how an algorithm&#39;s performance degrades as the input size grows</em>. Generally speaking, we are interested in two performance metrics:</p>
<ul>
<li><strong>Time complexity:</strong> as the input grows infinitely large, how much <em>longer</em> does it take to solve the problem?</li>
<li><strong>Space complexity:</strong> as the input grows infinitely large, how much <em>more memory</em> does it take to solve the problem?</li>
</ul>
<p>Let&#39;s consider the list searching example, where we have a list of numbers and we want to return a boolean indicating whether or not the number is in the list. We could write the following pseudocode algorithm:</p>
<pre><code>function search(target, list):
  for each item in list:
    if item == target then return true
  return false
</code></pre><p>This algorithm starts at the beginning of the list and visits each item until we&#39;ve either found our target, or gone through the entire list. In the <em>best case</em> scenario, our target is the first item of the list -- then the loop only runs once, and we&#39;re done!</p>
<p>In general, though, the best case scenario is not very interesting. When we compare algorithms, we&#39;re interested in how they perform when the rubber meets the road -- that is, complexity in the <em>worst case</em> scenario.</p>
<p>For our example, the worst case scenario is when our list <em>doesn&#39;t contain our target element at all</em>. When this happens, our algorithm can&#39;t just find a match and terminate early. It has to check the entire list before calling it quits.</p>
<blockquote>
<p><strong>Note:</strong> When we say &quot;best case&quot; and &quot;worst case,&quot; we mean &quot;the best case <em>in terms of our algorithm&#39;s performance</em>,&quot; not the best case in terms of external meaning we&#39;ve ascribed to the results.</p>
<p>In the previous example, our worst case isn&#39;t the worst case because we didn&#39;t find what we were looking for -- it&#39;s the worst case because <em>the algorithm has to go through the entire list instead of being able to terminate early</em>. Likewise, our best case isn&#39;t the best because we found our target; it&#39;s the best because we only had to run the loop once.</p>
<p>An algorithm&#39;s <em>performance</em> (e.g. how long it takes to finish, or how much memory it uses) is totally separate from any subjective valuation we might attach to its <em>output</em> (e.g. &quot;found = good and not found = bad!!!!!1&quot;). For example, suppose we are searching a DNA sequence to determine whether a mutation exists. In the Real World, the ideal output is a result of &quot;not found,&quot; but in Algorithm World, this is the worst case performance scenario. Conversely, finding the mutation at the very beginning of the patient&#39;s DNA sequence is the &quot;best case&quot; from a performance standpoint, even though it would be a bad outcome in real life.</p>
<p>In short, &quot;best&quot; and &quot;worst case&quot; describe an algorithm&#39;s <em>inputs</em>, not its <em>outputs</em>.</p>
</blockquote>
<p>Now we&#39;ve established that in the worst case, our algorithm has to go through the entire input list. Intuitively, it takes longer to go through a long list than a short one. But roughly how much longer? This is the <strong>time complexity</strong> of our algorithm, and we use something called <strong>asymptotic notation</strong> to describe this value.</p>
<h3>Big O analysis</h3><p>Recall the worst case input requires our algorithm to visit each list item exactly once:</p>
<pre><code>function search(list, target):
  for each item in list:
    if item == target then return true
  return false
</code></pre><p>Suppose our input list contains $n$ items. Since our algorithm spends the same amount of time on each item, the total worst-case runtime is <em>directly proportional to the length of the input list</em>. In other words, if we express runtime $T$ as a function of input size $n$, then $T(n)$ is <em>linear</em>. <strong>Equivalently, we say that the time complexity of our algorithm is $O(n)$</strong>, pronounced &quot;oh-N&quot; or &quot;oh of N.&quot;</p>
<p>This is an example of <strong>big O notation</strong>, which is the most common way computer scientists talk about algorithmic efficiency. Big O notation expresses an <em>upper bound</em> on a function. In particular, it expresses an upper bound in terms of <em>another function</em> that will always be <em>greater than or equal to</em> the first function.</p>
<p>When we analyze algorithms, we are interested in $T(n)$, or runtime as a function of input size. Specifically, we care about how quickly $T(n)$ grows. Saying $T$ is $O(n)$ means that $T$ grows <em>no faster than</em> the linear function $n$. This is the same as saying the linear function $n$ is an <em>upper bound</em> on $T$.</p>
<blockquote>
<p><strong>Note:</strong> Big O expresses relationships between different functions of some variable, as that variable grows infinitely large. In other words, it compares the <em>asymptotic behavior</em> of functions.</p>
<p>There are a lot of ways to talk about big O mathematically. For two functions $f(n)$ and $g(n)$, the following expressions are more or less equivalent:</p>
<ul>
<li>$f(n)$ is $O \big( g(n) \big)$, or $f(n) = O \big( g(n) \big)$, or $f$ is $O(g)$</li>
<li>$f$ grows (slower than/no faster than) $g$</li>
<li>$f$ is upper bounded by $g$, or $g$ is an upper bound on $f$</li>
<li>$f(n) \leq kg(n)$, for some constant factor $k$</li>
<li>as $n$ approaches infinity, the ratio of $f$ to $g$ is finite</li>
<li>$\lim_{x\to\infty} \Big| \frac{f(x)}{g(x)} \Big| &lt; \infty$</li>
</ul>
</blockquote>
<p>It turns out that $O(n)$ complexity is really good, relatively speaking (remember that big O is all about classifying functions relative to other functions). If you think about it, it&#39;s pretty reasonable to expect a function&#39;s runtime to increase proportionally to the input size. In fact, to do any better, our runtime would need to <em>not increase at all</em> -- that is, we would need $T(n)$ to remain <em>constant</em> as $n$ grows infinitely large.</p>
<p>Such constant $T(n)$s do exist, but are typically very simple. Consider the following function, which just adds 3 to a number:</p>
<pre><code>function add3(n):
  return n + 3
</code></pre><p>No matter how large $n$ gets, <code>add3</code> takes the same amount of time to complete -- in other words, its runtime $T(n)$ is constant. <strong>We say that <code>add3</code> is constant time, or $O(1)$</strong>, pronounced &quot;oh-one&quot; or &quot;oh of one.&quot;</p>
<p>Now we&#39;ve seen two common <strong>complexity classes</strong>: $O(1)$ and $O(n)$. Let&#39;s meet some others:</p>
<table>
<thead>
<tr>
<th>Notation</th>
<th>Complexity class</th>
<th>Judgment</th>
</tr>
</thead>
<tbody>
<tr>
<td>$O(1)$</td>
<td>Constant</td>
<td>Awesome</td>
</tr>
<tr>
<td>$O(\log{n})$</td>
<td>Logarithmic</td>
<td>Good</td>
</tr>
<tr>
<td>$O(n)$</td>
<td>Linear</td>
<td>Decent</td>
</tr>
<tr>
<td>$O(n^2)$</td>
<td>Quadratic</td>
<td>Bad</td>
</tr>
<tr>
<td>$O(2^n)$</td>
<td>Exponential</td>
<td>Awful</td>
</tr>
</tbody>
</table>
<p>Big O is all about classifying functions into groups like these. To understand big O in a nutshell, let&#39;s momentarily throw back to plotting graphs of linear and quadratic equations in middle school. Here is a graph of the functions $f(x) = x$ and $g(x) = x^2$.</p>
<p><img src="http://i.imgur.com/SnMxLRQ.png" height="400px"></p>
<p>This graph makes it easy to see that for all values of $x &gt; 1$, the quadratic function $x^2$ dominates the linear function $x$. As $x$ grows infinitely large, not only will $x^2$ keep growing, but the <em>gap</em> between $x^2$ and $x$ will keep growing as well.</p>
<p>Compare this behavior to the difference between linear functions $x$ and $3x$. Although $3x$ always dominates $x$, the gap between the functions remains the same: We can intuitively say that linear and quadratic functions are in different &quot;growth classes,&quot; whereas, say, the functions $x$ and $3x$ are in the same linear growth class, even though $3x$ will always dominate $x$.</p>
<p>Here&#39;s the idea in a nutshell: &quot;Let&#39;s divide all the functions, ever, into classes that describe approximately how fast a function grows, in terms of the closest general upper bound.</p>
<p>We can get a pretty good idea of how big O complexity works by comparing these growth rates for very large values of $n$:</p>
<p><img alt="Graph showing the relative growth rates for each complexity class" src="http://bigocheatsheet.com/img/big-o-complexity.png" title="Comparison of complexity classes"></p>
<p>Namely, notice how each function grows successively faster than its predecessor. Remember that we are using big O to measure the growth of our algorithm&#39;s running time $T(n)$. So,</p>
<ul>
<li>the faster $T(n)$ grows,</li>
<li>the longer our algorithm takes, and</li>
<li>the worse it performs.</li>
</ul>
<h3>Complexity classes</h3><p>Look at the above graph again. We&#39;ve already observed that each function grows successively faster than its predecessor -- the linear function $n$ grows strictly faster than the constant function $1$, the quadratic function $n^2$ grows strictly faster than the linear function $n$, and so on.</p>
<p>We can actually make an even stronger statement. Each successive function grows faster than not only its predecessor, but <em>all preceding functions</em>. That is,</p>
<p>$$1 \leq \log{n} \leq n \leq n^2 \leq 2^n$$</p>
<p>as $n$ grows infinitely large.</p>
<p>Recall that big O notation expresses some function as the <em>upper bound</em> of another. (For instance, saying $T(n) = O(n^2)$ means that $T(n)$ is upper-bounded by the function $n^2$.)</p>
<p>If we say that a function $f$ is $O(g)$, we are basically saying that $f$ has an upper bound $g$, or</p>
<p>$$f \leq g.$$</p>
<p>Now, suppose this bound $g$ is itself $O(h)$, meaning $g$ has upper bound $h$:</p>
<p>$$g \leq h.$$</p>
<p>Chaining the two statements together, we can write $h$ as the upper bound of $f$:</p>
<p>$$f \leq g \leq h$$</p>
<p>Now drop the $g$ to relate $f$ and $h$:</p>
<p>$$f \leq h$$</p>
<p>Translating this into big O notation, $f$ is $O(h)$, in addition to being $O(g)$.</p>
<p>This seemingly trivial example illustrates an important property: a function $f$ is $O(\text{all functions larger than $f$})$.</p>
<p>For instance, if $T(n)$ is $O(n)$, it&#39;s technically <em>also</em> $O(n^2)$, $O(n^3)$, $O(2^n)$, etc. But since we&#39;re trying to describe $T(n)$ with an upper bound, we want to provide the lowest bound we can, in order to best approximate $T(n)$. So in practice, we just say $T(n)$ is $O(n)$.</p>
<p>The following image neatly summarizes the relationship between complexity classes of functions.</p>
<p><img alt="Big O defines a partial ordering on the set of all functions" src="http://i.imgur.com/L5i1bjE.png" title="Big O complexity classes" height="400px"></p>
<h3>Other asymptotic relationships</h3><p>Although big O is the most common notation used for comparing functions, there are two others worth knowing.</p>
<p>The &quot;opposite&quot; of big O is <strong>big $\Omega$</strong> (pronounced &quot;big omega&quot;), which defines a function as a lower bound on another function. The following statements are logically equivalent:</p>
<ul>
<li>$f$ is $\Omega(g)$, pronounced &quot;$f$ is omega of $g$&quot;</li>
<li>$f$ grows at least as quickly as $g$</li>
<li>$g$ is a lower bound on $f$</li>
<li>$f(n) \geq kg(n)$, for some constant $k$</li>
</ul>
<p>For example, we know that $n$ is $O(\log{n})$ -- that is, $\log{n}$ upper bounds the function $n$. Conversely, $\log{n}$ is $\Omega(n)$ -- that is, the function $n$ lower bounds $\log{n}$.</p>
<p>Finally, <strong>big $\Theta$</strong> (pronounced &quot;big theta&quot;) defines a function as &quot;tight bound&quot; to another function. Two functions are &quot;tight bound&quot; (and thus $\Theta$ of one another) if they grow the same way/at the same rate. By definition, $f$ is $\Theta(g)$ when we can &quot;sandwich&quot; $f$ between bounds $k_1 g$ and $k_2 g$, for some constants $k_1$ and $k_2$.</p>
<p>For example, saying &quot;$T(n)$ is $\Theta(n^2)$&quot; means there exist constants $k_1$ and $k_2$ such that</p>
<p>$$k_1 n^2 \leq T(n) \leq k_2 n^2$$</p>
<p>which means that as $n$ grows infinitely large, the function $T(n)$ will always stay between $n^2$ scaled by some constant $k_1$, and $n^2$ scaled by some constant $k_2$.</p>
<p>Usually these constants are different, and the two versions of $n^2$ &quot;sandwich&quot; the graph of $T(n)$. If the constants are the same, then $T(n)$ is exactly $kn^2$. Finally, if they are the same value <em>and</em> that value happens to be 1, then $T(n)$ is exactly $n^2$.</p>
<blockquote>
<p><strong>Note:</strong> When we talk about algorithm performance, it&#39;s easy to confuse &quot;bound terminology&quot; (upper bound $O$, lower bound $\Omega$, and tight bound $\Theta$) with &quot;case terminology&quot; (best case, worst case, and average case).</p>
<p>Recall that we use &quot;(best/worst/average) case&quot; to describe the features of an algorithm&#39;s input(s), and the corresponding impact upon the algorithm&#39;s performance. Each &quot;case&quot; produces a different $T(n)$ for some algorithm, depending on how favorable the inputs are.</p>
<p>By contrast, we use asymptotic notation and &quot;(upper/lower/tight) bounds&quot; to describe functions in terms of other functions. Here, we already know the algorithm&#39;s performance $T(n)$, but we don&#39;t care how we got this function; we just want to classify its growth rate in relation to other functions.</p>
<p>How do these ideas relate? If we let $T(n)$ denote the time complexity/performance of an algorithm, then the &quot;best case&quot; input <em>minimizes</em> $T(n)$ -- which minimizes the upper, lower, <em>and</em> tight bounds on $T(n)$. Conversely, the &quot;worst case&quot; input <em>maximizes</em> $T(n)$, which maximizes all three bounds.</p>
<p>As a slightly more concrete example, consider our original searching algorithm:</p>
<ul>
<li><strong>Best case:</strong> our target is the <em>first item in the list</em>. Then no matter how large the list, the algorithm has the same runtime, because we only need to check the first item. So $T(n) = O(1)$.<ul>
<li>But we could <em>also</em> write $T(n) = \Theta(1)$ or $T(n) = \Omega(1)$, because asymptotic notation just describes the function $T(n)$ in terms of other functions.</li>
</ul>
</li>
<li><strong>Worst case:</strong> our target <em>isn&#39;t in the list</em>. Then the algorithm&#39;s runtime grows proportionally to the size of the list. So $T(n) = O(n)$.<ul>
<li>But we could <em>also</em> write $T(n) = \Omega(1)$, because $T$ grows faster than any constant-valued function -- in other words, the constant function is a <em>lower bound</em> on $T(n)$.</li>
</ul>
</li>
</ul>
</blockquote></article></main><footer><p>Written by Sarah Lim (slim at northwestern dot edu)</p></footer></div><script src="https://dl.dropboxusercontent.com/u/14218448/assets/prism.js" type="text/javascript"></script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEnvironments: true
},
TeX: {
    Macros: {
      pr: ['\\Pr \\big[ #1 \\big]', 1],
      Ex: ['\\mathrm{E} \\big[ #1 \\big]', 1],
      set: ['\\{ #1 \\}', 1],
      Sum: ['\sum_{ #1 }^{ #2 }', 2],
      sdef: ['#1 := \\{ #2 \\}', 2],
      etc: [', \\ldots, '],
      d: ['\\text{dist}'],
      vx: ['\\vec{ x }'],
      vy: ['\\vec{ y }'],
      vl: ['\\|\\vec{ #1 }\\|', 1],
      al: ['\\begin{align} #1 \\end{align}', 1],
      under: ['\\underbrace{ #1 }_\\text{ #2 }', 2],
      multf: ['#1 = \\begin{cases} #2 & \\text{if } #3 \\\\ #4 & \\text{if } #5 \\end{cases}', 5]
    }
}});</script></body></html>